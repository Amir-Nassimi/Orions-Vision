{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d623ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchreid\\reid\\metrics\\rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import torchreid\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from threading import Thread\n",
    "from datetime import datetime\n",
    "from math import hypot as Dist\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff18c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620d9ca7",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b2ffd",
   "metadata": {},
   "source": [
    "## Person_ReIdentification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa505b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datamanager = torchreid.data.ImageDataManager(\n",
    "#     root=\"reid-data\",\n",
    "#     sources=\"market1501\",\n",
    "#     targets=\"market1501\",\n",
    "#     height=256,\n",
    "#     width=128,\n",
    "#     batch_size_train=32,\n",
    "#     batch_size_test=100,\n",
    "#     transforms=[\"random_flip\", \"random_crop\"]\n",
    "# )\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.Resize((256, 128))\n",
    "#     transforms.Resize((160, 64))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09603f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = torchreid.models.build_model(\n",
    "#     name=\"hacnn\",\n",
    "    name=\"xception\",\n",
    "    num_classes=751, #datamanager.num_train_pids\n",
    "    loss=\"softmax\",\n",
    "    pretrained=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c506eba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Xception(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (block1): Block(\n",
       "    (skip): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (skipbn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (rep): Sequential(\n",
       "      (0): SeparableConv2d(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "        (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): SeparableConv2d(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block2): Block(\n",
       "    (skip): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (skipbn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block3): Block(\n",
       "    (skip): Conv2d(256, 728, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (skipbn): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (pointwise): Conv2d(256, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (block4): Block(\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block5): Block(\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block6): Block(\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block7): Block(\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block8): Block(\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block9): Block(\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block10): Block(\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block11): Block(\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (8): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (block12): Block(\n",
       "    (skip): Conv2d(728, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (skipbn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (rep): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): SeparableConv2d(\n",
       "        (conv1): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
       "        (pointwise): Conv2d(728, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "  )\n",
       "  (conv3): SeparableConv2d(\n",
       "    (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "    (pointwise): Conv2d(1024, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (bn3): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): SeparableConv2d(\n",
       "    (conv1): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "    (pointwise): Conv2d(1536, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (bn4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (global_avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Linear(in_features=2048, out_features=751, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint = torch.load('./model.pth.tar-100')\n",
    "checkpoint = torch.load('./model.pth.tar-80')\n",
    "# checkpoint = torch.load('./log5/HaCNN/model/model.pth.tar-120')\n",
    "myModel.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "myModel = myModel.to(device)\n",
    "myModel.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b6de9",
   "metadata": {},
   "source": [
    "## Person Detection Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00419341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\a.nasimi/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-10-2 Python-3.9.12 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce RTX 3060, 12287MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5x6 summary: 574 layers, 140730220 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "things_model = torch.hub.load('ultralytics/yolov5', 'custom', path='./yolov5x6.pt',device=f'{device}:0' if device=='cuda' else f'{device}')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7c39e",
   "metadata": {},
   "source": [
    "# Camera Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1e07594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadedCamera(object):\n",
    "    def __init__(self, source='cam:Ashkan123@192.168.172.199:88/videoMain'):\n",
    "\n",
    "        self.capture = cv2.VideoCapture(f'rtsp://{source}')\n",
    "\n",
    "        self.thread = Thread(target = self.update, args = ())\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "\n",
    "        self.status = False\n",
    "        self.frame  = None\n",
    "\n",
    "    def update(self):\n",
    "        while True:\n",
    "            if self.capture.isOpened(): (self.status, self.frame) = self.capture.read()\n",
    "            else: continue\n",
    "\n",
    "    def grab_frame(self):\n",
    "        if self.status: return self.frame\n",
    "        else: return None \n",
    "    \n",
    "    def end_frame(self):\n",
    "        self.capture.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d98f4",
   "metadata": {},
   "source": [
    "# Handeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a050b7",
   "metadata": {},
   "source": [
    "## PreValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "128393b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "count_skipped = 0\n",
    "\n",
    "thresh = 8.5\n",
    "thresh_param = 3\n",
    "thresh_on_estimate = int(5e2)\n",
    "\n",
    "old_frame,last_labels = {},[]\n",
    "counter,skipped_frame,skipped_count = 0,0,0\n",
    "\n",
    "ROI_Region,ROI_Person = [0,0,0,0],[0,0,0,0]\n",
    "img_original,database,database_unknowns,rgb_colors = None,[],[],{}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fff4ac",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7819d6",
   "metadata": {},
   "source": [
    "### Person Detection Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5fe378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pre_Proc_Light(img):\n",
    "    lightness = np.mean(img)\n",
    "    if lightness < 95:\n",
    "        img_yuv = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "        return cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)\n",
    "    else: return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d19844b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import tensorflow as tf\n",
    "# from tf_bodypix.api import download_model, load_model, BodyPixModelPaths\n",
    "\n",
    "# bodypix_model = load_model(download_model(BodyPixModelPaths.MOBILENET_FLOAT_50_STRIDE_16))\n",
    "\n",
    "# def Blurring(image):\n",
    "\n",
    "# #     image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
    "#     result = bodypix_model.predict_single(image)\n",
    "\n",
    "#     mask = result.get_mask(threshold=0.20)\n",
    "#     res = cv2.bitwise_and(image,image,mask = np.uint8(mask))\n",
    "    \n",
    "#     blur = cv2.blur(image,(45,45),0)\n",
    "#     out = image.copy()\n",
    "\n",
    "#     sharpen_kernel = np.array([[0,-1,0], [-1,5,-1], [0,-1,0]])\n",
    "#     sharpen = cv2.filter2D(out, -1, sharpen_kernel)\n",
    "\n",
    "#     sharpen[res==0] = blur[res==0]\n",
    " \n",
    "#     return sharpen\n",
    "    \n",
    "\n",
    "# #     # colored mask (separate colour for each body part)\n",
    "# #     colored_mask = result.get_colored_part_mask(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bf219e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Person_Detection(img,img_orig,ROI_Region):\n",
    "    imgs = []\n",
    "    save_flag = True\n",
    "    img_original = img_orig.copy()\n",
    "    \n",
    "    now_time = datetime.now().strftime(\"%Y_%m_%d---%I_%M_%S---%p\")\n",
    "    img = np.array(Image.fromarray(img))\n",
    "    img = cv2.cvtColor(np.array(img),cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    with torch.no_grad(): results = things_model(img)\n",
    "\n",
    "    if len(results.xyxy[0]) == 0: pass\n",
    "    else:\n",
    "        img_original = cv2.cvtColor(np.array(img_original),cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        for indx,res in enumerate(results.xyxy[0]):\n",
    "            \n",
    "            predict = results.pandas().xyxy[0]['name'][indx]\n",
    "            \n",
    "            if predict == 'person': pass\n",
    "            else: continue\n",
    "\n",
    "            res = np.array(res.detach().cpu())\n",
    "            if res[4]*100 < 45: save_flag = False\n",
    "            else: \n",
    "                x = round(res[0])\n",
    "                y = round(res[1])\n",
    "                w = round(res[2])\n",
    "                h = round(res[3])\n",
    "                \n",
    "                imgs.append((img[y:h,x:w],[x,y,h,w]))\n",
    "        \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a65f3c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crop_Persons(imgs,new_frame):\n",
    "    indx_ = 0\n",
    "    for img,cordinations in imgs:\n",
    "        x = cordinations[0]\n",
    "        y = cordinations[1]\n",
    "        h = cordinations[2]\n",
    "        w = cordinations[3]\n",
    "\n",
    "#         img_ = Blurring(img)\n",
    "        img_ = img.copy()\n",
    "\n",
    "        if img_.shape[0] < 300: continue\n",
    "        elif img_.shape[1] < 140: continue\n",
    "        else:          \n",
    "            new_frame[indx_] = {\n",
    "                'Img':img_,\n",
    "                'Status':'-',\n",
    "                'Label':'Unknown',\n",
    "                'Coordination':[x,y,w,h],\n",
    "                'Last_PID_Run':datetime.now()\n",
    "            }\n",
    "            indx_ += 1\n",
    "            \n",
    "    return new_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5dfed",
   "metadata": {},
   "source": [
    "### Person Re-Identification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76a834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_Similarity(distns,thresh):\n",
    "    try: indx,dist = min(distns, key=lambda x: x[1])\n",
    "    except ValueError: dist = 9e9\n",
    "\n",
    "    if dist < thresh: return indx,dist,True\n",
    "    else: return 'Unknown',None,False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90af72c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Person_ReIdentification(img,flag_known=True,flag_unknown=False):\n",
    "    img = cv2.cvtColor(img.copy(),cv2.COLOR_BGR2RGB)\n",
    "    img = Pre_Proc_Light(img)\n",
    "    img_ = transform(img)\n",
    "    x = myModel(img_.unsqueeze(0).float().cuda()).detach().cpu()\n",
    "    \n",
    "    if flag_known:\n",
    "        distns = []\n",
    "        for data in database:\n",
    "            for embd in data['Embds']:\n",
    "                dist = torch.cdist(x, embd, p=2.0)[0][0]\n",
    "                distns.append((data['Id'],dist))\n",
    "        return distns,x\n",
    "    else: pass\n",
    "            \n",
    "    if flag_unknown:\n",
    "        distns = []\n",
    "        for data in database_unknowns:\n",
    "            for embd in data['Embds']:\n",
    "                dist = torch.cdist(x, embd, p=2.0)[0][0]\n",
    "                distns.append((data['Id'],dist))\n",
    "        return distns,x\n",
    "    else: pass\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "966089b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Identify_Persons(persons,old_frame_info,labels):\n",
    "    for person_num in persons:\n",
    "        distns,emb = Person_ReIdentification(persons[person_num]['Img'],flag_known=True,flag_unknown=False)\n",
    "        label,dist,flag = Check_Similarity(distns,thresh)\n",
    "\n",
    "        if flag: \n",
    "            persons[person_num],_,_,_,_,labels,old_frame,persons,flag_state = Check_And_Go(persons[person_num],label,emb,'Known',dist,labels,old_frame_info,persons)\n",
    "            if flag_state:\n",
    "                labels.append(label)\n",
    "                persons[person_num]['Label'] = label\n",
    "                persons[person_num]['Status'] = 'Identified'\n",
    "                persons[person_num]['Last_PID_Run'] = datetime.now()\n",
    "            else: continue\n",
    "\n",
    "        else:\n",
    "            distns,emb = Person_ReIdentification(persons[person_num]['Img'],flag_known=False,flag_unknown=True)\n",
    "            label,dist,flag = Check_Similarity(distns,thresh)\n",
    "\n",
    "            if flag: \n",
    "                persons[person_num],_,_,_,_,labels,old_frame,persons,flag_state = Check_And_Go(persons[person_num],label,emb,'Unknown',dist,labels,old_frame_info,persons)\n",
    "                \n",
    "                if flag_state:\n",
    "                    labels.append(label)\n",
    "                    persons[person_num]['Label'] = label\n",
    "                    persons[person_num]['Status'] = 'Identified'\n",
    "                    persons[person_num]['Last_PID_Run'] = datetime.now()\n",
    "                else: continue\n",
    "\n",
    "            else:\n",
    "                label,dist,flag = Check_Similarity(distns,thresh+thresh_param)\n",
    "\n",
    "                if flag:\n",
    "                    persons[person_num],_,_,_,_,labels,old_frame,persons,flag_state = Check_And_Go(persons[person_num],label,emb,'Unknown',dist,labels,old_frame_info,persons)\n",
    "#                     labels,persons,flag_state = Check_And_Go(persons[person_num],label,emb,'Unknown',dist,labels,old_frame_info,persons)\n",
    "                    if flag_state:\n",
    "                        labels.append(label)\n",
    "                        persons[person_num]['Label'] = label\n",
    "                        persons[person_num]['Status'] = 'Identified'\n",
    "                        persons[person_num]['Last_PID_Run'] = datetime.now()\n",
    "                    else: continue\n",
    "\n",
    "                else: \n",
    "                    label = Add_Person_To_DB(emb,False)\n",
    "                    \n",
    "                    labels.append(label)\n",
    "                    persons[person_num]['Label'] = label\n",
    "                    persons[person_num]['Status'] = 'Identified'\n",
    "                    persons[person_num]['Last_PID_Run'] = datetime.now()\n",
    "\n",
    "    return persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0364fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_And_Go(current_person,current_label,current_emb,state,current_dist,prev_labels,old_frame,new_persons):\n",
    "    if current_label in prev_labels:\n",
    "        print(\"1\")\n",
    "        flag_ = False\n",
    "        for indx in old_frame:\n",
    "            if current_label == old_frame[indx]['Label']:\n",
    "                flag_ = True\n",
    "                break\n",
    "            else: continue\n",
    "                 \n",
    "        if flag_:  # آیا برچسب مذکور در فریم قبلی دیده شده است؟\n",
    "            print(\"4\")\n",
    "            if 'Unknown' in current_label:\n",
    "                label = Add_Person_To_DB(current_emb,False)\n",
    "\n",
    "                prev_labels.append(label)\n",
    "                current_person['Label'] = label\n",
    "                current_person['Status'] = 'Identified'\n",
    "                current_person['Last_PID_Run'] = datetime.now()\n",
    "\n",
    "                return current_person,label,current_emb,'Unknown',9e9,prev_labels,old_frame,new_persons,False\n",
    "            \n",
    "            else:\n",
    "                distns,emb = Person_ReIdentification(current_person['Img'],flag_known=False,flag_unknown=True)\n",
    "                label,dist,flag = Check_Similarity(distns,thresh)\n",
    "\n",
    "                print(f\"1 - current label: {current_label} - label: {label}\")\n",
    "\n",
    "                if flag: return Check_And_Go(current_person,label,emb,'Unknown',dist,prev_labels,old_frame,new_persons)\n",
    "                else:\n",
    "                    label,dist,flag = Check_Similarity(distns,thresh+thresh_param)\n",
    "                    print(f\"1 - current label: {current_label} - label: {label}\")\n",
    "\n",
    "                    if flag: return Check_And_Go(current_person,label,emb,'Unknown',dist,prev_labels,old_frame,new_persons)\n",
    "                    else: \n",
    "                        print(\"5\")\n",
    "                        label = Add_Person_To_DB(emb,False)\n",
    "\n",
    "                        prev_labels.append(label)\n",
    "                        current_person['Label'] = label\n",
    "                        current_person['Status'] = 'Identified'\n",
    "                        current_person['Last_PID_Run'] = datetime.now()\n",
    "\n",
    "                        return current_person,label,emb,'Unknown',9e9,prev_labels,old_frame,new_persons,False         \n",
    "                        \n",
    "        else:   \n",
    "            for indx in new_persons:\n",
    "                if new_persons[indx]['Label'] == current_label: break\n",
    "                else: continue\n",
    "                \n",
    "            print(\"10\")\n",
    "            if ('Unknown' in current_label) and not ('Unknown' == current_label):\n",
    "                print(\"6\")\n",
    "                distns,emb = Person_ReIdentification(new_persons[indx]['Img'],flag_known=True,flag_unknown=False)\n",
    "                label,dist,flag = Check_Similarity(distns,thresh)\n",
    "                \n",
    "                if flag: print(\"7\")\n",
    "                else:\n",
    "                    distns,emb = Person_ReIdentification(new_persons[indx]['Img'],flag_known=False,flag_unknown=True)\n",
    "                    label,dist,flag = Check_Similarity(distns,thresh)\n",
    "                    print(f\"2 - current label: {current_label} - label: {label}\")\n",
    "                    \n",
    "                    if flag: pass\n",
    "                    else: \n",
    "                        label,dist,flag = Check_Similarity(distns,thresh+thresh_param)\n",
    "                        print(f\"2 - current label: {current_label} - label: {label}\")\n",
    "                        \n",
    "                if dist < current_dist:     \n",
    "                    print(\"8\")\n",
    "                    label = Add_Person_To_DB(current_emb,False)\n",
    "\n",
    "                    prev_labels.append(label)\n",
    "                    current_person['Label'] = label\n",
    "                    current_person['Status'] = 'Identified'\n",
    "                    current_person['Last_PID_Run'] = datetime.now()\n",
    "\n",
    "                    return current_person,label,current_emb,'Unknown',9e9,prev_labels,old_frame,new_persons,False  \n",
    "\n",
    "                else:\n",
    "                    print(\"9\")\n",
    "                    current_person['Label'] = label\n",
    "                    current_person['Status'] = 'Identified'\n",
    "                    current_person['Last_PID_Run'] = datetime.now()\n",
    "\n",
    "                    label = Add_Person_To_DB(emb,False)\n",
    "\n",
    "                    prev_labels.append(label)\n",
    "                    new_persons[indx]['Label'] = label\n",
    "                    new_persons[indx]['Status'] = 'Identified'\n",
    "                    new_persons[indx]['Last_PID_Run'] = datetime.now()\n",
    "\n",
    "                    return current_person,label,emb,'Unknown',9e9,prev_labels,old_frame,new_persons,False  \n",
    "                \n",
    "            else:\n",
    "                print(\"10\")\n",
    "                persons_dists = []\n",
    "                persons_dists.append(current_dist)\n",
    "                \n",
    "                distns,_ = Person_ReIdentification(new_persons[indx]['Img'],flag_known=True,flag_unknown=False)\n",
    "                try: indx,dist = min(distns, key=lambda x: x[1])\n",
    "                except ValueError: dist = 9e9\n",
    "                    \n",
    "                persons_dists.append(dist)   \n",
    "                    \n",
    "                arg = np.argmin(persons_dists)\n",
    "                if arg == 0:\n",
    "                    current_person['Label'] = label\n",
    "                    current_person['Status'] = 'Identified'\n",
    "                    current_person['Last_PID_Run'] = datetime.now()\n",
    "                    \n",
    "                    person_to_investigate = new_persons[indx]\n",
    "                    \n",
    "                else: person_to_investigate = current_person\n",
    "                    \n",
    "                distns,emb = Person_ReIdentification(person_to_investigate['Img'],flag_known=False,flag_unknown=True)\n",
    "                label,dist,flag = Check_Similarity(distns,thresh)\n",
    "                print(f\"3 - current label: {current_label} - label: {label}\")\n",
    "                if flag: return Check_And_Go(person_to_investigate,label,emb,'Unknown',dist,prev_labels,old_frame,new_persons)\n",
    "                else:\n",
    "                    label,dist,flag = Check_Similarity(distns,thresh+thresh_param)\n",
    "                    print(f\"3 - current label: {current_label} - label: {label}\")\n",
    "                    if flag: return Check_And_Go(person_to_investigate,label,emb,'Unknown',dist,prev_labels,old_frame,new_persons)\n",
    "                    else: \n",
    "                        label = Add_Person_To_DB(emb,False)\n",
    "                        \n",
    "                        prev_labels.append(label)\n",
    "                        person_to_investigate['Label'] = label\n",
    "                        person_to_investigate['Status'] = 'Identified'\n",
    "                        person_to_investigate['Last_PID_Run'] = datetime.now()\n",
    "                        \n",
    "                        return current_person,label,emb,'Unknown',9e9,prev_labels,old_frame,new_persons,False\n",
    "    else: \n",
    "        print(\"2\")\n",
    "        prev_labels.append(current_label)\n",
    "        return current_person,current_label,current_emb,state,current_dist,prev_labels,old_frame,new_persons,True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982f10e",
   "metadata": {},
   "source": [
    "### Normal Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f235187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Write_On_Image(label,cor,img_original):\n",
    "    ROI_Region = [0,0,0,0]\n",
    "    try: color = rgb_colors[label]\n",
    "    except KeyError:\n",
    "        r = random.randrange(1, 256, 20)\n",
    "        g = random.randrange(1, 256, 20)\n",
    "        b = random.randrange(1, 256, 20)\n",
    "        color = (r,g,b)\n",
    "        rgb_colors[label] = color\n",
    "\n",
    "    img_original = cv2.rectangle(np.array(img_original), (cor[0]+ROI_Region[0], cor[1]+ROI_Region[1],abs(cor[2]-(cor[0])),abs(cor[3]-(cor[1]))), color, 2)\n",
    "    img_original = cv2.putText(np.array(img_original), f\"No: {label}\", (cor[0]+ROI_Region[0], cor[1]+ROI_Region[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, color,2,cv2.LINE_AA)\n",
    "    \n",
    "    return img_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fd4de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_Person_To_DB(person_emb,flag,indx=None):\n",
    "    if flag:\n",
    "        if indx is None: indx = f'{len(database) + 1}'\n",
    "        else: pass\n",
    "        \n",
    "        data = {\n",
    "            'Id':f'{indx}',\n",
    "            'Embds':[person_emb],\n",
    "            'Last_Update':time.gmtime()\n",
    "        }\n",
    "        \n",
    "        database.append(data)\n",
    "        return indx\n",
    "    \n",
    "    else:\n",
    "        if indx is None: indx = f'{len(database_unknowns) + 1}'\n",
    "        else: \n",
    "            if 'Unknown ' in indx: indx = indx.replace('Unknown ','')\n",
    "            else: pass\n",
    "        \n",
    "        data = {\n",
    "            'Id':f'Unknown {indx}',\n",
    "            'Embds':[person_emb],\n",
    "            'Last_Update':time.gmtime()\n",
    "        }\n",
    "        \n",
    "        database_unknowns.append(data)\n",
    "        return f'Unknown {indx}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9288187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Databse_Update(persons):\n",
    "    def Time_Passed(current_time,second=2):\n",
    "        oldsecond = current_time[5]\n",
    "        currentsecond = time.gmtime()[5]\n",
    "\n",
    "        if ((currentsecond - oldsecond) >= second): return True\n",
    "        else: return False\n",
    "    \n",
    "    for data in database:\n",
    "        if Time_Passed(data['Last_Update']):\n",
    "            flag_ = False\n",
    "            for indx in persons:\n",
    "                if persons[indx]['Label'] == data['Id']: \n",
    "                    flag_ = True\n",
    "                    break\n",
    "                else: continue\n",
    "\n",
    "            if flag_:\n",
    "                if len(data['Embds']) > (15+1): data['Embds'].pop(0)\n",
    "                else: pass\n",
    "\n",
    "                new_embd = Person_ReIdentification(persons[indx]['Img'],flag_known=False,flag_unknown=False)\n",
    "                data['Embds'].append(new_embd)\n",
    "                data['Last_Update'] = time.gmtime()\n",
    "                \n",
    "            else: continue\n",
    "        else: continue\n",
    "\n",
    "    for data in database_unknowns:\n",
    "        if Time_Passed(data['Last_Update']):\n",
    "            flag_ = False\n",
    "            for indx in persons:\n",
    "                if persons[indx]['Label'] == data['Id']: \n",
    "                    flag_ = True\n",
    "                    break\n",
    "                else: continue\n",
    "\n",
    "            if flag_:\n",
    "                if len(data['Embds']) > (15+1): data['Embds'].pop(0)\n",
    "                else: pass\n",
    "                \n",
    "                new_embd = Person_ReIdentification(persons[indx]['Img'],flag_known=False,flag_unknown=False)\n",
    "                data['Embds'].append(new_embd)\n",
    "                data['Last_Update'] = time.gmtime()\n",
    "                \n",
    "            else: continue\n",
    "        else: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8f8f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge_Frame_Dictionaries(*keywords):\n",
    "    new_farme,counter = {},0\n",
    "    \n",
    "    for key in keywords:\n",
    "        for indx in key:\n",
    "            if key[indx]['Status'] == '-': continue\n",
    "            else:\n",
    "                new_farme[counter] = key[indx].copy()\n",
    "                counter += 1  \n",
    "                \n",
    "    return new_farme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6fc6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_Speed(*keywords):\n",
    "    def Dotproduct(v1, v2):\n",
    "        return sum((a*b) for a, b in zip(v1, v2))\n",
    "\n",
    "    def Length(v):\n",
    "        return math.sqrt(Dotproduct(v, v))\n",
    "    \n",
    "    def Angle(v1, v2):\n",
    "        return math.acos(Dotproduct(v1, v2) / (Length(v1) * Length(v2)))\n",
    "\n",
    "    point1 = [(keywords[0][0] + keywords[0][2]) / 2,(keywords[0][1] + keywords[0][3]) / 2]\n",
    "    point2 = [(keywords[1][0] + keywords[1][2]) / 2,(keywords[1][1] + keywords[1][3]) / 2]\n",
    "    \n",
    "    angle1 = Angle(point1,[1e-100,1e-100])\n",
    "    angle2 = Angle(point2,[1e-100,1e-100])\n",
    "    \n",
    "#     return (angle2 - angle1) / (keywords[2] / 25)\n",
    "    \n",
    "    try: return (angle2 - angle1) * (180/np.pi)\n",
    "    except IndexError: return (angle2 - angle1) * (np.pi / 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcba20e6",
   "metadata": {},
   "source": [
    "# Main Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a246e",
   "metadata": {},
   "source": [
    "## Add Person to Known DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07208d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter_ = 0\n",
    "# for indx,pth in enumerate(glob('./Sample1*')):\n",
    "#     if counter_ > 16: break\n",
    "#     img = cv2.imread(pth)\n",
    "#     imgs = Person_Detection(img,img,ROI_Region)\n",
    "    \n",
    "# #     img = Blurring(imgs[0][0])\n",
    "#     img = imgs[0][0]\n",
    "#     emb = Person_ReIdentification(img,False,False)\n",
    "    \n",
    "#     if indx == 0: indx_ = Add_Person_To_DB(emb,flag=True,indx='Fadaeian')\n",
    "#     else: Add_Person_To_DB(emb,flag=True,indx=f'{indx_}')\n",
    "        \n",
    "#     counter_ += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd282e",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4072ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamer = ThreadedCamera('rtsp:Ashkan321@172.16.60.121')\n",
    "streamer = ThreadedCamera('rtsp:Ashkan123@172.16.60.123:554/Streaming/Channels/201')\n",
    "\n",
    "# cap = cv2.VideoCapture('./video_1.mp4')\n",
    "\n",
    "# width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "# height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# writer = cv2.VideoWriter('./video_1400_test_known_No_Skip_4_speed.mp4', cv2.VideoWriter_fourcc(*'DIVX'), 25, (width,height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c0965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28**************************\n",
      "29**************************\n",
      "32**************************\n",
      "33**************************\n",
      "34**************************\n",
      "35**************************\n",
      "36**************************\n",
      "37**************************\n",
      "38**************************\n",
      "39**************************\n",
      "40**************************\n",
      "41**************************\n",
      "42**************************\n",
      "43**************************\n",
      "44**************************\n",
      "45**************************\n",
      "46**************************\n",
      "47**************************\n",
      "48**************************\n",
      "49**************************\n",
      "50**************************\n",
      "51**************************\n",
      "52**************************\n",
      "53**************************\n",
      "54**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55**************************\n",
      "56**************************\n",
      "57**************************\n",
      "58**************************\n",
      "59**************************\n",
      "60**************************\n",
      "61**************************\n",
      "62**************************\n",
      "63**************************\n",
      "64**************************\n",
      "65**************************\n",
      "66**************************\n",
      "67**************************\n",
      "68**************************\n",
      "69**************************\n",
      "70**************************\n",
      "71**************************\n",
      "72**************************\n",
      "73**************************\n",
      "74**************************\n",
      "75**************************\n",
      "76**************************\n",
      "77**************************\n",
      "78**************************\n",
      "79**************************\n",
      "80**************************\n",
      "81**************************\n",
      "82**************************\n",
      "83**************************\n",
      "84**************************\n",
      "85**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86**************************\n",
      "87**************************\n",
      "88**************************\n",
      "89**************************\n",
      "90**************************\n",
      "91**************************\n",
      "92**************************\n",
      "93**************************\n",
      "94**************************\n",
      "95**************************\n",
      "96**************************\n",
      "97**************************\n",
      "98**************************\n",
      "99**************************\n",
      "100**************************\n",
      "101**************************\n",
      "102**************************\n",
      "103**************************\n",
      "104**************************\n",
      "105**************************\n",
      "106**************************\n",
      "107**************************\n",
      "108**************************\n",
      "109**************************\n",
      "110**************************\n",
      "111**************************\n",
      "112**************************\n",
      "113**************************\n",
      "114**************************\n",
      "115**************************\n",
      "116**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117**************************\n",
      "118**************************\n",
      "119**************************\n",
      "120**************************\n",
      "121**************************\n",
      "122**************************\n",
      "123**************************\n",
      "124**************************\n",
      "125**************************\n",
      "126**************************\n",
      "127**************************\n",
      "128**************************\n",
      "129**************************\n",
      "130**************************\n",
      "131**************************\n",
      "132**************************\n",
      "133**************************\n",
      "134**************************\n",
      "135**************************\n",
      "136**************************\n",
      "137**************************\n",
      "138**************************\n",
      "139**************************\n",
      "140**************************\n",
      "141**************************\n",
      "142**************************\n",
      "143**************************\n",
      "144**************************\n",
      "145**************************\n",
      "146**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147**************************\n",
      "148**************************\n",
      "149**************************\n",
      "150**************************\n",
      "151**************************\n",
      "152**************************\n",
      "153**************************\n",
      "154**************************\n",
      "155**************************\n",
      "156**************************\n",
      "157**************************\n",
      "158**************************\n",
      "159**************************\n",
      "160**************************\n",
      "161**************************\n",
      "162**************************\n",
      "163**************************\n",
      "164**************************\n",
      "165**************************\n",
      "166**************************\n",
      "167**************************\n",
      "168**************************\n",
      "169**************************\n",
      "170**************************\n",
      "171**************************\n",
      "172**************************\n",
      "173**************************\n",
      "174**************************\n",
      "175**************************\n",
      "176**************************\n",
      "177**************************\n",
      "178**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179**************************\n",
      "180**************************\n",
      "181**************************\n",
      "182**************************\n",
      "183**************************\n",
      "184**************************\n",
      "185**************************\n",
      "186**************************\n",
      "187**************************\n",
      "188**************************\n",
      "189**************************\n",
      "190**************************\n",
      "191**************************\n",
      "192**************************\n",
      "193**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215**************************\n",
      "216**************************\n",
      "217**************************\n",
      "218**************************\n",
      "219**************************\n",
      "220**************************\n",
      "221**************************\n",
      "222**************************\n",
      "223**************************\n",
      "224**************************\n",
      "225**************************\n",
      "226**************************\n",
      "227**************************\n",
      "228**************************\n",
      "229**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243**************************\n",
      "244**************************\n",
      "245**************************\n",
      "246**************************\n",
      "247**************************\n",
      "248**************************\n",
      "249**************************\n",
      "250**************************\n",
      "251**************************\n",
      "252**************************\n",
      "253**************************\n",
      "254**************************\n",
      "255**************************\n",
      "256**************************\n",
      "257**************************\n",
      "258**************************\n",
      "259**************************\n",
      "260**************************\n",
      "261**************************\n",
      "262**************************\n",
      "263**************************\n",
      "264**************************\n",
      "265**************************\n",
      "266**************************\n",
      "267**************************\n",
      "268**************************\n",
      "269**************************\n",
      "270**************************\n",
      "271**************************\n",
      "272**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273**************************\n",
      "274**************************\n",
      "275**************************\n",
      "276**************************\n",
      "277**************************\n",
      "278**************************\n",
      "279**************************\n",
      "280**************************\n",
      "281**************************\n",
      "282**************************\n",
      "283**************************\n",
      "284**************************\n",
      "285**************************\n",
      "286**************************\n",
      "287**************************\n",
      "288**************************\n",
      "289**************************\n",
      "290**************************\n",
      "291**************************\n",
      "292**************************\n",
      "293**************************\n",
      "294**************************\n",
      "295**************************\n",
      "296**************************\n",
      "297**************************\n",
      "298**************************\n",
      "299**************************\n",
      "300**************************\n",
      "301**************************\n",
      "302**************************\n",
      "303**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304**************************\n",
      "305**************************\n",
      "306**************************\n",
      "307**************************\n",
      "308**************************\n",
      "309**************************\n",
      "310**************************\n",
      "311**************************\n",
      "312**************************\n",
      "313**************************\n",
      "314**************************\n",
      "315**************************\n",
      "316**************************\n",
      "317**************************\n",
      "318**************************\n",
      "319**************************\n",
      "320**************************\n",
      "321**************************\n",
      "322**************************\n",
      "323**************************\n",
      "324**************************\n",
      "325**************************\n",
      "326**************************\n",
      "327**************************\n",
      "328**************************\n",
      "329**************************\n",
      "330**************************\n",
      "331**************************\n",
      "332**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333**************************\n",
      "334**************************\n",
      "335**************************\n",
      "336**************************\n",
      "337**************************\n",
      "338**************************\n",
      "339**************************\n",
      "340**************************\n",
      "341**************************\n",
      "342**************************\n",
      "343**************************\n",
      "344**************************\n",
      "345**************************\n",
      "346**************************\n",
      "347**************************\n",
      "348**************************\n",
      "349**************************\n",
      "350**************************\n",
      "351**************************\n",
      "352**************************\n",
      "353**************************\n",
      "354**************************\n",
      "355**************************\n",
      "356**************************\n",
      "357**************************\n",
      "358**************************\n",
      "359**************************\n",
      "360**************************\n",
      "361**************************\n",
      "362**************************\n",
      "363**************************\n",
      "364**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365**************************\n",
      "366**************************\n",
      "367**************************\n",
      "368**************************\n",
      "369**************************\n",
      "370**************************\n",
      "371**************************\n",
      "372**************************\n",
      "373**************************\n",
      "374**************************\n",
      "375**************************\n",
      "376**************************\n",
      "377**************************\n",
      "378**************************\n",
      "379**************************\n",
      "380**************************\n",
      "381**************************\n",
      "382**************************\n",
      "383**************************\n",
      "384**************************\n",
      "385**************************\n",
      "386**************************\n",
      "387**************************\n",
      "388**************************\n",
      "389**************************\n",
      "390**************************\n",
      "391**************************\n",
      "392**************************\n",
      "393**************************\n",
      "394**************************\n",
      "395**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396**************************\n",
      "397**************************\n",
      "398**************************\n",
      "399**************************\n",
      "400**************************\n",
      "401**************************\n",
      "402**************************\n",
      "403**************************\n",
      "404**************************\n",
      "405**************************\n",
      "406**************************\n",
      "407**************************\n",
      "408**************************\n",
      "409**************************\n",
      "410**************************\n",
      "411**************************\n",
      "413**************************\n",
      "414**************************\n",
      "415**************************\n",
      "416**************************\n",
      "417**************************\n",
      "418**************************\n",
      "419**************************\n",
      "420**************************\n",
      "421**************************\n",
      "422**************************\n",
      "423**************************\n",
      "424**************************\n",
      "425**************************\n",
      "426**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427**************************\n",
      "428**************************\n",
      "429**************************\n",
      "430**************************\n",
      "431**************************\n",
      "432**************************\n",
      "433**************************\n",
      "434**************************\n",
      "435**************************\n",
      "436**************************\n",
      "437**************************\n",
      "438**************************\n",
      "439**************************\n",
      "440**************************\n",
      "441**************************\n",
      "442**************************\n",
      "443**************************\n",
      "444**************************\n",
      "445**************************\n",
      "446**************************\n",
      "447**************************\n",
      "448**************************\n",
      "449**************************\n",
      "450**************************\n",
      "451**************************\n",
      "452**************************\n",
      "453**************************\n",
      "454**************************\n",
      "455**************************\n",
      "456**************************\n",
      "457**************************\n",
      "458**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459**************************\n",
      "460**************************\n",
      "461**************************\n",
      "462**************************\n",
      "463**************************\n",
      "464**************************\n",
      "465**************************\n",
      "466**************************\n",
      "467**************************\n",
      "468**************************\n",
      "469**************************\n",
      "470**************************\n",
      "471**************************\n",
      "472**************************\n",
      "473**************************\n",
      "474**************************\n",
      "475**************************\n",
      "476**************************\n",
      "477**************************\n",
      "478**************************\n",
      "479**************************\n",
      "480**************************\n",
      "481**************************\n",
      "482**************************\n",
      "483**************************\n",
      "484**************************\n",
      "485**************************\n",
      "486**************************\n",
      "487**************************\n",
      "488**************************\n",
      "489**************************\n",
      "490**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491**************************\n",
      "492**************************\n",
      "493**************************\n",
      "494**************************\n",
      "495**************************\n",
      "496**************************\n",
      "497**************************\n",
      "498**************************\n",
      "499**************************\n",
      "2\n",
      "500**************************\n",
      "501**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502**************************\n",
      "503**************************\n",
      "504**************************\n",
      "505**************************\n",
      "506**************************\n",
      "507**************************\n",
      "Hryyy\n",
      "508**************************\n",
      "509**************************\n",
      "510**************************\n",
      "511**************************\n",
      "512**************************\n",
      "513**************************\n",
      "514**************************\n",
      "515**************************\n",
      "516**************************\n",
      "517**************************\n",
      "518**************************\n",
      "519**************************\n",
      "520**************************\n",
      "521**************************\n",
      "522**************************\n",
      "523**************************\n",
      "524**************************\n",
      "525**************************\n",
      "526**************************\n",
      "527**************************\n",
      "528**************************\n",
      "529**************************\n",
      "530**************************\n",
      "531**************************\n",
      "532**************************\n",
      "533**************************\n",
      "534**************************\n",
      "535**************************\n",
      "536**************************\n",
      "537**************************\n",
      "538**************************\n",
      "539**************************\n",
      "540**************************\n",
      "541**************************\n",
      "542**************************\n",
      "543**************************\n",
      "544**************************\n",
      "545**************************\n",
      "546**************************\n",
      "547**************************\n",
      "548**************************\n",
      "549**************************\n",
      "550**************************\n",
      "551**************************\n",
      "552**************************\n",
      "553**************************\n",
      "554**************************\n",
      "555**************************\n",
      "556**************************\n",
      "557**************************\n",
      "558**************************\n",
      "559**************************\n",
      "560**************************\n",
      "561**************************\n",
      "562**************************\n",
      "563**************************\n",
      "2\n",
      "564**************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\AI\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565**************************\n",
      "Hryyy\n",
      "566**************************\n",
      "567**************************\n",
      "568**************************\n",
      "569**************************\n",
      "570**************************\n",
      "571**************************\n",
      "572**************************\n",
      "573**************************\n",
      "574**************************\n",
      "575**************************\n",
      "576**************************\n",
      "577**************************\n",
      "578**************************\n",
      "579**************************\n",
      "580**************************\n",
      "581**************************\n",
      "582**************************\n",
      "583**************************\n",
      "584**************************\n",
      "585**************************\n",
      "586**************************\n",
      "587**************************\n",
      "588**************************\n",
      "589**************************\n",
      "590**************************\n",
      "591**************************\n",
      "592**************************\n",
      "593**************************\n",
      "594**************************\n",
      "595**************************\n",
      "596**************************\n",
      "597**************************\n",
      "598**************************\n",
      "599**************************\n",
      "600**************************\n",
      "601**************************\n",
      "602**************************\n",
      "603**************************\n",
      "604**************************\n",
      "605**************************\n",
      "606**************************\n",
      "607**************************\n",
      "608**************************\n",
      "609**************************\n",
      "610**************************\n",
      "611**************************\n",
      "612**************************\n",
      "613**************************\n",
      "614**************************\n",
      "615**************************\n",
      "616**************************\n",
      "617**************************\n",
      "618**************************\n",
      "619**************************\n",
      "620**************************\n",
      "621**************************\n",
      "622**************************\n",
      "623**************************\n",
      "624**************************\n",
      "625**************************\n",
      "626**************************\n",
      "627**************************\n",
      "628**************************\n",
      "629**************************\n",
      "630**************************\n",
      "631**************************\n",
      "632**************************\n",
      "633**************************\n",
      "634**************************\n",
      "635**************************\n",
      "636**************************\n",
      "637**************************\n",
      "638**************************\n",
      "639**************************\n",
      "640**************************\n",
      "641**************************\n",
      "642**************************\n",
      "643**************************\n",
      "644**************************\n",
      "645**************************\n",
      "646**************************\n",
      "647**************************\n",
      "648**************************\n",
      "649**************************\n",
      "650**************************\n",
      "651**************************\n",
      "652**************************\n",
      "653**************************\n",
      "654**************************\n",
      "655**************************\n",
      "656**************************\n",
      "657**************************\n",
      "658**************************\n",
      "659**************************\n",
      "660**************************\n",
      "661**************************\n",
      "662**************************\n",
      "663**************************\n",
      "664**************************\n",
      "665**************************\n",
      "666**************************\n",
      "667**************************\n",
      "668**************************\n",
      "669**************************\n",
      "670**************************\n",
      "671**************************\n",
      "672**************************\n",
      "673**************************\n",
      "674**************************\n",
      "675**************************\n",
      "676**************************\n",
      "677**************************\n",
      "678**************************\n",
      "679**************************\n",
      "680**************************\n",
      "681**************************\n",
      "682**************************\n",
      "683**************************\n",
      "684**************************\n",
      "685**************************\n",
      "686**************************\n",
      "687**************************\n",
      "688**************************\n",
      "689**************************\n",
      "690**************************\n",
      "691**************************\n",
      "692**************************\n",
      "693**************************\n",
      "694**************************\n",
      "695**************************\n",
      "696**************************\n",
      "697**************************\n",
      "698**************************\n",
      "699**************************\n",
      "700**************************\n",
      "701**************************\n",
      "702**************************\n",
      "703**************************\n",
      "704**************************\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    x,y,h,w = 0,0,0,0\n",
    "    \n",
    "    new_frame,all_labels = {},[]\n",
    "    \n",
    "#     ret,frame = cap.read()    \n",
    "    frame = streamer.grab_frame()\n",
    "    frame = cv2.resize(frame,(1250,750))\n",
    "\n",
    "    if counter != 0 and (counter % 5) == 0: \n",
    "        count_skipped += 1\n",
    "        if count_skipped > 1: \n",
    "            counter += 1\n",
    "            count_skipped = 0\n",
    "            \n",
    "        else: continue\n",
    "            \n",
    "    else: counter += 1\n",
    "\n",
    "    try: img_original = frame.copy()\n",
    "    except AttributeError: break\n",
    "\n",
    "    imgs = Person_Detection(frame,img_original,ROI_Region)\n",
    "\n",
    "    if len(imgs) == 0: pass\n",
    "    else: new_frame = Crop_Persons(imgs,new_frame)\n",
    "        \n",
    "    if len(new_frame) == 0:\n",
    "        \n",
    "#         writer.write(img_original)\n",
    "#         cv2.imwrite(f\"./Pics/{counter}.jpg\",img_original)\n",
    "        cv2.imshow('Person_ReIdentification', np.array(img_original))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "            cv2.destroyAllWindows()\n",
    "            streamer.end_frame()\n",
    "            break  \n",
    "        else: continue        \n",
    "        \n",
    "        print(f\"{counter}--**************************\")\n",
    "        continue\n",
    "\n",
    "    else: pass\n",
    "    \n",
    "    if len(old_frame) == 0 or (counter % thresh_on_estimate) == 0:\n",
    "        new_frame = Identify_Persons(new_frame,[],all_labels)\n",
    "\n",
    "    elif len(old_frame) <= len(new_frame):\n",
    "        for i in old_frame:\n",
    "            distances_compares,flag_ = [],False\n",
    "            cor_old = [(old_frame[i]['Coordination'][0]+old_frame[i]['Coordination'][2])/2,(old_frame[i]['Coordination'][1]+old_frame[i]['Coordination'][3])/2]\n",
    "            for j in new_frame:\n",
    "                cor_current = [(new_frame[j]['Coordination'][0]+new_frame[j]['Coordination'][2])/2,(new_frame[j]['Coordination'][1]+new_frame[j]['Coordination'][3])/2]     \n",
    "\n",
    "                dist_ = Dist(cor_old[0]-cor_current[0],cor_old[1]-cor_current[1])\n",
    "                \n",
    "#                 speed_ = Calculate_Speed([old_frame[i]['Coordination'][0],\n",
    "#                           old_frame[i]['Coordination'][1],\n",
    "#                           old_frame[i]['Coordination'][2],\n",
    "#                           old_frame[i]['Coordination'][3]],[new_frame[j]['Coordination'][0],\n",
    "#                                                             new_frame[j]['Coordination'][1],\n",
    "#                                                             new_frame[j]['Coordination'][2],\n",
    "#                                                             new_frame[j]['Coordination'][3]],1)\n",
    "                \n",
    "                \n",
    "                distances_compares.append(dist_)\n",
    "\n",
    "            j = np.argmin(distances_compares)\n",
    "            \n",
    "            new_frame[j]['Status'] = 'Estimate'\n",
    "            new_frame[j]['Label'] = old_frame[i]['Label']\n",
    "            new_frame[j]['Last_PID_Run'] = datetime.now()\n",
    "            \n",
    "            all_labels.append(old_frame[i]['Label'])\n",
    "            \n",
    "        new_frame_2 = {}\n",
    "        for indx in new_frame:\n",
    "            if new_frame[indx]['Status'] == '-': new_frame_2[indx] = new_frame[indx].copy()\n",
    "            else: continue\n",
    "        \n",
    "        new_frame_2 = Identify_Persons(new_frame_2,old_frame,all_labels)\n",
    "        \n",
    "        new_frame = Merge_Frame_Dictionaries(new_frame,new_frame_2)\n",
    "        \n",
    "    else: \n",
    "        for i in new_frame:\n",
    "            distances_compares,flag_ = [],False\n",
    "            cor_old = [(new_frame[i]['Coordination'][0]+new_frame[i]['Coordination'][2])/2,(new_frame[i]['Coordination'][1]+new_frame[i]['Coordination'][3])/2]\n",
    "            for j in old_frame:\n",
    "                cor_current = [(old_frame[j]['Coordination'][0]+old_frame[j]['Coordination'][2])/2,(old_frame[j]['Coordination'][1]+old_frame[j]['Coordination'][3])/2]     \n",
    "                \n",
    "                dist_ = Dist(cor_old[0]-cor_current[0],cor_old[1]-cor_current[1])\n",
    "#                 speed_ = Calculate_Speed([new_frame[i]['Coordination'][0],\n",
    "#                                           new_frame[i]['Coordination'][1],\n",
    "#                                           new_frame[i]['Coordination'][2],\n",
    "#                                           new_frame[i]['Coordination'][3]],[old_frame[j]['Coordination'][0],\n",
    "#                                                                             old_frame[j]['Coordination'][1],\n",
    "#                                                                             old_frame[j]['Coordination'][2],\n",
    "#                                                                             old_frame[j]['Coordination'][3]],1)\n",
    "                distances_compares.append(dist_)\n",
    "\n",
    "            j = np.argmin(distances_compares)\n",
    "\n",
    "            new_frame[i]['Status'] = 'Estimate'\n",
    "            new_frame[i]['Label'] = old_frame[j]['Label']\n",
    "            new_frame[i]['Last_PID_Run'] = datetime.now()\n",
    "            \n",
    "            print(\"Hryyy\")\n",
    "        \n",
    "    \n",
    "    for i in new_frame:\n",
    "        img_original = Write_On_Image(f\"{new_frame[i]['Label']}\" ,new_frame[i]['Coordination'],img_original)\n",
    "    \n",
    "    \n",
    "    Databse_Update(new_frame)\n",
    "    old_frame = new_frame.copy()\n",
    "\n",
    "    \n",
    "#     writer.write(img_original)\n",
    "#     cv2.imwrite(f\"./Pics/{counter}.jpg\",img_original)\n",
    "    \n",
    "    cv2.imshow('Person_ReIdentification', np.array(img_original))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        cv2.destroyAllWindows()\n",
    "        streamer.end_frame()\n",
    "        break  \n",
    "        \n",
    "    print(f\"{counter}**************************\")\n",
    "    \n",
    "# cap.release()\n",
    "# writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024f986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Check_And_Go(current_person,current_label,current_emb,state,current_dist,prev_labels,old_frame,new_persons):\n",
    "#     if current_label in prev_labels:\n",
    "#         print(\"1\")\n",
    "#         flag_ = False\n",
    "#         for old_indx in old_frame:\n",
    "#             if current_label == old_frame[old_indx]['Label']:\n",
    "#                 flag_ = True\n",
    "#                 break\n",
    "#             else: continue\n",
    "                \n",
    "#         for indx in new_persons:\n",
    "#             if new_persons[indx]['Label'] == current_label: break\n",
    "#             else: continue\n",
    "                    \n",
    "#         if flag_:  # آیا برچسب مذکور در فریم قبلی دیده شده است؟\n",
    "#             print(\"3\")\n",
    "#             cor_current = [(current_person['Coordination'][0]+current_person['Coordination'][2])/2,(current_person['Coordination'][1]+current_person['Coordination'][3])/2]     \n",
    "#             cor_prev = [(new_persons[indx]['Coordination'][0]+new_persons[indx]['Coordination'][2])/2,(new_persons[indx]['Coordination'][1]+new_persons[indx]['Coordination'][3])/2]\n",
    "#             cor_ref = [(old_frame[old_indx]['Coordination'][0]+old_frame[old_indx]['Coordination'][2])/2,(old_frame[old_indx]['Coordination'][1]+old_frame[old_indx]['Coordination'][3])/2]\n",
    "            \n",
    "#             dist_1 = Dist(cor_ref[0]-cor_prev[0],cor_ref[1]-cor_prev[1])\n",
    "#             dist_2 = Dist(cor_ref[0]-cor_current[0],cor_ref[1]-cor_current[1])\n",
    "            \n",
    "#             if dist_1 < dist_2:\n",
    "#                 print(\"4\")\n",
    "#                 if 'Unknown' in current_label:\n",
    "#                     label = Add_Person_To_DB(current_emb,False)\n",
    "                    \n",
    "#                     prev_labels.append(label)\n",
    "#                     current_person['Label'] = label\n",
    "#                     current_person['Status'] = 'Identified'\n",
    "#                     current_person['Last_PID_Run'] = datetime.now()\n",
    "                    \n",
    "#                     return current_person,label,current_emb,'Unknown',9e9,prev_labels,old_frame,new_persons,False\n",
    "                \n",
    "#                 else:\n",
    "#                     distns,emb = Person_ReIdentification(current_person['Img'],flag_known=False,flag_unknown=True)\n",
    "#                     label,dist,flag = Check_Similarity(distns,thresh)\n",
    "                    \n",
    "#                     print(f\"1 - current label: {current_label} - label: {label}\")\n",
    "                    \n",
    "#                     if flag: return Check_And_Go(current_person,label,emb,'Unknown',dist,prev_labels,old_frame,new_persons)\n",
    "#                     else:\n",
    "#                         label,dist,flag = Check_Similarity(distns,thresh+thresh_param)\n",
    "#                         print(f\"1 - current label: {current_label} - label: {label}\")\n",
    "\n",
    "#                         if flag: return Check_And_Go(current_person,label,emb,'Unknown',dist,prev_labels,old_frame,new_persons)\n",
    "#                         else: \n",
    "#                             print(\"5\")\n",
    "#                             label = Add_Person_To_DB(emb,False)\n",
    "                            \n",
    "#                             prev_labels.append(label)\n",
    "#                             current_person['Label'] = label\n",
    "#                             current_person['Status'] = 'Identified'\n",
    "#                             current_person['Last_PID_Run'] = datetime.now()\n",
    "                            \n",
    "#                             return current_person,label,emb,'Unknown',9e9,prev_labels,old_frame,new_persons,False\n",
    "                        \n",
    "#         else:    \n",
    "#             print(\"10\")\n",
    "#             if ('Unknown' in current_label) and not ('Unknown' == current_label):\n",
    "#                 print(\"6\")\n",
    "#                 distns,emb = Person_ReIdentification(new_persons[indx]['Img'],flag_known=True,flag_unknown=False)\n",
    "#                 label,dist,flag = Check_Similarity(distns,thresh)\n",
    "                \n",
    "#                 if flag: print(\"7\")\n",
    "#                 else:\n",
    "#                     distns,emb = Person_ReIdentification(new_persons[indx]['Img'],flag_known=False,flag_unknown=True)\n",
    "#                     label,dist,flag = Check_Similarity(distns,thresh)\n",
    "#                     print(f\"2 - current label: {current_label} - label: {label}\")\n",
    "                    \n",
    "#                     if flag: pass\n",
    "#                     else: \n",
    "#                         label,dist,flag = Check_Similarity(distns,thresh+thresh_param)\n",
    "#                         print(f\"2 - current label: {current_label} - label: {label}\")\n",
    "                        \n",
    "#                 if dist < current_dist:     \n",
    "#                     print(\"8\")\n",
    "#                     label = Add_Person_To_DB(current_emb,False)\n",
    "\n",
    "#                     prev_labels.append(label)\n",
    "#                     current_person['Label'] = label\n",
    "#                     current_person['Status'] = 'Identified'\n",
    "#                     current_person['Last_PID_Run'] = datetime.now()\n",
    "\n",
    "#                     return current_person,label,current_emb,'Unknown',9e9,prev_labels,old_frame,new_persons,False  \n",
    "\n",
    "#                 else:\n",
    "#                     print(\"9\")\n",
    "#                     current_person['Label'] = label\n",
    "#                     current_person['Status'] = 'Identified'\n",
    "#                     current_person['Last_PID_Run'] = datetime.now()\n",
    "\n",
    "#                     label = Add_Person_To_DB(emb,False)\n",
    "\n",
    "#                     prev_labels.append(label)\n",
    "#                     new_persons[indx]['Label'] = label\n",
    "#                     new_persons[indx]['Status'] = 'Identified'\n",
    "#                     new_persons[indx]['Last_PID_Run'] = datetime.now()\n",
    "\n",
    "#                     return current_person,label,emb,'Unknown',9e9,prev_labels,old_frame,new_persons,False  \n",
    "                \n",
    "#             else:\n",
    "#                 print(\"10\")\n",
    "#                 persons_dists = []\n",
    "#                 persons_dists.append(current_dist)\n",
    "                \n",
    "#                 distns,_ = Person_ReIdentification(new_persons[indx]['Img'],flag_known=True,flag_unknown=False)\n",
    "#                 try: indx,dist = min(distns, key=lambda x: x[1])\n",
    "#                 except ValueError: dist = 9e9\n",
    "                    \n",
    "#                 persons_dists.append(dist)   \n",
    "                    \n",
    "#                 arg = np.argmin(persons_dists)\n",
    "#                 if arg == 0:\n",
    "#                     current_person['Label'] = label\n",
    "#                     current_person['Status'] = 'Identified'\n",
    "#                     current_person['Last_PID_Run'] = datetime.now()\n",
    "                    \n",
    "#                     person_to_investigate = new_persons[indx]\n",
    "                    \n",
    "#                 else: person_to_investigate = current_person\n",
    "                    \n",
    "#                 distns,emb = Person_ReIdentification(person_to_investigate['Img'],flag_known=False,flag_unknown=True)\n",
    "#                 label,dist,flag = Check_Similarity(distns,thresh)\n",
    "#                 print(f\"3 - current label: {current_label} - label: {label}\")\n",
    "#                 if flag: return Check_And_Go(person_to_investigate,label,emb,'Unknown',dist,prev_labels,old_frame,new_persons)\n",
    "#                 else:\n",
    "#                     label,dist,flag = Check_Similarity(distns,thresh+thresh_param)\n",
    "#                     print(f\"3 - current label: {current_label} - label: {label}\")\n",
    "#                     if flag: return Check_And_Go(person_to_investigate,label,emb,'Unknown',dist,prev_labels,old_frame,new_persons)\n",
    "#                     else: \n",
    "#                         label = Add_Person_To_DB(emb,False)\n",
    "                        \n",
    "#                         prev_labels.append(label)\n",
    "#                         person_to_investigate['Label'] = label\n",
    "#                         person_to_investigate['Status'] = 'Identified'\n",
    "#                         person_to_investigate['Last_PID_Run'] = datetime.now()\n",
    "                        \n",
    "#                         return current_person,label,emb,'Unknown',9e9,prev_labels,old_frame,new_persons,False\n",
    "#     else: \n",
    "#         print(\"2\")\n",
    "#         prev_labels.append(current_label)\n",
    "#         return current_person,current_label,current_emb,state,current_dist,prev_labels,old_frame,new_persons,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabaebd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
